---
title: "Regresja liniowa"
author: "Paweł Wieczyński"
format: html
editor: visual
---

```{r}
if(!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse)
theme_set(theme_bw())
options(scipen = 99)
```

## Metoda najmniejszych kwadratów

Poniższy zbiór danych zwiera $n = 12$ realizacji wektora losowego $(X, Y)$, gdzie:

-   $Y$ oznacza wzrost dzieci w cm

-   $X$ oznacza wiek dzieci w miesiącach.

Zakładamy, że istnieje funkcja $f$ taka, że $Y = f(X) + \epsilon$, a naszym celem jest znalezienie funkcji $\hat{f}$, która przybliża funkcję $f$ w pewien ***optymalny*** sposób.

```{r}
df = read.csv('datasets\\age_height.csv')
ggplot(df, aes(x = age, y = height)) + geom_point()
```

W modelu regresji liniowej zakładamy, że istnieją stałe parametry $\beta_0, \beta_1 \in \mathbb{R}$, takie że $Y = \beta_0 + \beta_1 X + \epsilon$. Musimy zatem znaleźć $\hat{\beta_0}, \hat{\beta_0}$.

```{r}
#tbd wykres z kilkoma przykladami beta0 oraz beta1 i liniami przerywanymi z residualsami
```

### $d>1$ zmiennych objaśniających

W ogólnym przypadku, gdy mamy $n$ obserwacji oraz $d$ zmiennych objaśniających, model regresji liniowej ma postać:

$$
Y = \beta_0 + \sum_{i=1}^d \beta_i X + \epsilon
$$

Szukamy wektor $d+1$ parametrów $\hat{\beta} = \left[ \hat{\beta_0}, \hat{\beta_1}, \dots, \hat{\beta_p} \right] \in \mathbb{R}^{d+1}$, takich, że

$$
\frac{1}{n} \sum_{i=1}^n \left[ \sum_{j=0}^d \left( \beta_j x_{ij} - y_i \right) \right]^2 \longrightarrow \min
$$

### Zapis macierzowy

W tym wypadku (i w wielu innych) warto zastosować zapis macierzowy. W tym wypadku mamy następujące macierze/wektory:

-   $\mathbf{X}$ - macierz danych wejściowych wymiaru $n \times (d+1)$; pierwsza kolumna tej macierzy zawiera same jedynki i będzie potrzebna do oszacowania parametru $\beta_0$

-   $\mathbf{y}$ - wektor danych wyjściowych wymiaru $n \times 1$

-   $\hat{\beta}$ - wektor szukanych parametrów wymiaru $(d+1) \times 1$

-   oraz funkcję straty, którą chcemy minimalizaować:

    $$
    L\left( \hat{\beta} \right) = \frac{1}{n} \left| \left| \ \mathbf{X} \hat{\beta} - \mathbf{y} \  \right| \right|^2 \longrightarrow \min
    $$

Liczymy gradient funkcji straty i przyrównujemy go do zera:

$$
\nabla L\left( \hat{\beta} \right) = \frac{2}{n} \mathbf{X}^\intercal \left( \mathbf{X} \hat{\beta} - \mathbf{y} \right) = 0
$$

Wymnażamy nawias i przenosimy na drugą stronę (*ćw. sprawdzić czy wymiary się zgadzają*):

$$
\mathbf{X}^\intercal \mathbf{X} \hat{\beta} = \mathbf{X}^\intercal \mathbf{y}
$$

Rozwiązujemy równanie macierzowe względem wektora $\hat{\beta}$:

$$
\hat{\beta} = \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \mathbf{y} = \mathbf{X}^\dagger \mathbf{y}
$$

Macierz $\mathbf{X}^\dagger$ nazywamy **macierzą pseudoodwrotną** macierzy $\mathbf{X}$.

W tej procedurze najbardziej złożonym krokiem jest odwrócenie macierzy $\mathbf{X}^\intercal \mathbf{X}$. W praktyce przyjmujemy, że:

-   im mniej zmiennych objaśniających $d$, tym łatwiej odwrócić macierz

-   jeśli ilość obserwacji $n$ jest wystarcająco duża, to $\mathbb{P} \left[ \ \mathbf{X}^\intercal \mathbf{X} \quad \text{jest odwracalna} \ \right] \simeq 1$.

### Przykład

```{r}
X = cbind(1, df$age)
y = df$height

X_pseudo_inverse = solve( t(X) %*% X ) %*% t(X)
beta = X_pseudo_inverse %*% y
beta
```

W języku R mamy gotowe polecenie `lm`:

```{r}
model = lm(height ~ age, data = df)
summary(model)
```

```{r}
ggplot(df, aes(x = age, y = height)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

## Metoda największej wiarygodności

Musimy poczynić pewne założenia na temat rozkładów. Załóżmy, że błędy losowe są niezależne oraz mają rozkład warunkowo normalny (TBD):

$$
\epsilon \ | \ X \overset{\text{iid}}{\sim} \mathcal{N} (0, \sigma^2_{\epsilon})
$$

Dla danej realizacji $\mathcal{D} = \lbrace (\mathbf{x}_i, y_i )\rbrace_{i=1}^n$ funkcja wiarygodności przyjmuje postać

$$
\mathcal{L}_n (\beta, \sigma^2 \ | \ \mathcal{D}) = 
\prod_{i=1}^n f_Y (y_i | \mathbf{x}_i) =  \\
= \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{1}{2} \frac{ \left( y_i - x_i \beta \right)^2}{\sigma^2} \right) = \\
= \frac{1}{\left( \sqrt{2\pi \sigma^2} \right)^n } \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n \left( y_i - x_i \beta \right)^2 \right)
$$

Logarytm funkcji wiarygodności przyjmuje wówczas postać

$$
l_n (\beta, \sigma^2 \ | \ \mathcal{D}) = \ln ( \mathcal{L}_n (\beta, \sigma^2 \ | \ \mathcal{D})) = \\
= \ln \left( \frac{1}{\left( \sqrt{2\pi \sigma^2} \right)^n }  \right) + \ln \left( \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n \left( y_i - x_i \beta \right)^2 \right) \right) = \\
= -\frac{n}{2} \ln \left( 2\pi \right) - \frac{n}{2} \ln \left( \sigma^2 \right) - \frac{1}{2\sigma^2} \sum_{i=1}^m \left( y_i - x_i\beta \right)^2
$$

Chcemy wyznaczyć parametry $\hat{\beta}$ oraz $\hat{\sigma}$, aby zmaksymalizowac logarytm wiarygodności. Liczymy zatem odpowiednie gradienty

$$
\nabla_{\beta} \ l_n (\beta, \sigma^2 \ | \ \mathcal{D}) = \frac{1}{\sigma^2} \sum_{i=1}^n \mathbf{x}_i^\intercal \left( y_i - \mathbf{x}_i \beta \right) = 
\frac{1}{\sigma^2} \left( \sum_{i=1}^n \mathbf{x}_i^\intercal y_i - \sum_{i=1}^n \mathbf{x}_i^\intercal \mathbf{x}_i \beta \right)
$$

$$
\frac{\partial }{\partial \sigma^2} l_n (\beta, \sigma^2 \ | \ \mathcal{D}) = 
-\frac{n}{2\sigma^2} - \left( \frac{1}{2} \sum_{i=1}^n \left( y_i - \mathbf{x}_i \beta \right)^2  \right) \frac{1}{\left( \sigma^2 \right)^2} = \\
= \frac{1}{2\sigma^2} \left( \frac{1}{\sigma^2} \sum_{i=1}^n \left( y_i - \mathbf{x}_i \beta \right)^2 - n \right)
$$

Przyrównując je do zera otrzymujemy

$$
\hat{\beta} = \left( \sum_{i=1}^n \mathbf{x}_i^\intercal \mathbf{x}_i \right)^{-1} \sum_{i=1}^n \mathbf{x}_i^\intercal y_i = \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal y = \mathbf{X}^\dagger y
$$

$$
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^m \left( y_i - \mathbf{x}_i \hat{\beta} \right)^2
$$

Widzimy zatem, że w przypadku regresji liniowej współczynniki $\hat{\beta}$ otrzymane za pomocą metody największej wiarygodności są takie same jak współczynniki otrzymane za pomoca metody najmniejszych kwadratów.
